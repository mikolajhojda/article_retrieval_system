{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7796657,"sourceType":"datasetVersion","datasetId":4564591},{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Article Retrieval System using Mistral7b and LangChain\n\nIn this notebook, I'll create an Article Retrieval System using Mistral7b and LangChain. I'll use also the Chroma vector database to store data, perform chunking and apply Retrieval Augmented Generation (RAG) in practice. I'll build both the searching of articles based on the query, but also question-answering (QA) system. The data that I'm going to use is a comprehensive collection of blog posts sourced from Medium, focusing specifically on articles published under the \"Towards Data Science\" publication. I'll also give five examples of different queries in searching articles, but also answering the questions.\n\n\n# Setup\n\nFirstly, it's necessary to install and then import all of the libraries and packages that we'll use in the project.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n!pip install -q -U transformers accelerate bitsandbytes langchain tiktoken sentence-transformers chromadb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-09T13:40:33.098169Z","iopub.execute_input":"2024-04-09T13:40:33.098802Z","iopub.status.idle":"2024-04-09T13:41:41.965500Z","shell.execute_reply.started":"2024-04-09T13:40:33.098767Z","shell.execute_reply":"2024-04-09T13:41:41.964416Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.2 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 29.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch import bfloat16\nimport transformers\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nimport langchain\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import DataFrameLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA, LLMChain\nfrom langchain.vectorstores import Chroma\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema.runnable import RunnablePassthrough\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:41:41.967586Z","iopub.execute_input":"2024-04-09T13:41:41.968317Z","iopub.status.idle":"2024-04-09T13:41:54.260699Z","shell.execute_reply.started":"2024-04-09T13:41:41.968277Z","shell.execute_reply":"2024-04-09T13:41:54.259762Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/1300-towards-datascience-medium-articles-dataset/medium.csv\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/config.json\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model-00002-of-00002.bin\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer.json\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer_config.json\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model.bin.index.json\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model-00001-of-00002.bin\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/special_tokens_map.json\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/.gitattributes\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer.model\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/generation_config.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* <b>torch.backends.cuda.enable_mem_efficient_sdp(False)</b>: This line is configuring Torch (PyTorch) to enable or disable memory-efficient structured data parallelism (SDP) for CUDA tensors. SDP is a technique for parallelizing computations across multiple GPUs by dividing data structures across them efficiently. By passing False as an argument, this line indicates that memory-efficient SDP should be disabled.\n\n* <b>torch.backends.cuda.enable_flash_sdp(False)</b>: Similarly, this line is configuring Torch to enable or disable flash structured data parallelism (SDP) for CUDA tensors. Flash SDP is another technique for parallelizing computations across multiple GPUs, potentially offering different trade-offs compared to memory-efficient SDP. By passing False as an argument, this line indicates that flash SDP should be disabled.\n\nI disabled this features, because in my case it's necessary to work with Mistral7b model further.","metadata":{}},{"cell_type":"code","source":"#configure torch\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:41:54.261908Z","iopub.execute_input":"2024-04-09T13:41:54.262346Z","iopub.status.idle":"2024-04-09T13:41:54.267508Z","shell.execute_reply.started":"2024-04-09T13:41:54.262317Z","shell.execute_reply":"2024-04-09T13:41:54.266351Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Data loading\n\nMy system indexes articles from the \"1300 Towards Data Science Medium Articles\", so I'll import the data and load team.","metadata":{}},{"cell_type":"code","source":"#import csv\ndf = pd.read_csv('/kaggle/input/1300-towards-datascience-medium-articles-dataset/medium.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:41:54.270438Z","iopub.execute_input":"2024-04-09T13:41:54.271584Z","iopub.status.idle":"2024-04-09T13:41:54.755817Z","shell.execute_reply.started":"2024-04-09T13:41:54.271539Z","shell.execute_reply":"2024-04-09T13:41:54.754959Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"I have the data stored in the form of pandas dataframe, so I want to use the DataframeLoader. It's important to mention that the page_content_column define the loader to identify which one is your id or page “title”.","metadata":{}},{"cell_type":"code","source":"#load data\narticles = DataFrameLoader(df, page_content_column = \"Title\")\ndocument = articles.load()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:41:54.756993Z","iopub.execute_input":"2024-04-09T13:41:54.757311Z","iopub.status.idle":"2024-04-09T13:41:55.056273Z","shell.execute_reply.started":"2024-04-09T13:41:54.757285Z","shell.execute_reply":"2024-04-09T13:41:55.055457Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"model_path=\"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n\n#initialize the model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype = torch.bfloat16,\n    device_map = \"auto\",\n    trust_remote_code = True\n)\n\n#initialize the tokenizer\ntokenizer=AutoTokenizer.from_pretrained(model_path)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:41:55.057491Z","iopub.execute_input":"2024-04-09T13:41:55.057870Z","iopub.status.idle":"2024-04-09T13:45:39.440163Z","shell.execute_reply.started":"2024-04-09T13:41:55.057838Z","shell.execute_reply":"2024-04-09T13:45:39.439094Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94c64ef098b749edb911b1fff9ffcf0e"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Chunking\n\nIt's a process of extracting meaningful phrases, or \"chunks,\" from a sentence based on its grammatical structure and parts of speech. It involves identifying and grouping together contiguous words or tokens that form a syntactic unit, typically consisting of a noun phrase, verb phrase, or prepositional phrase.\n\nIn my case chunking can help in text preprocessing, content selection or context segmentation.\n\nI use token-based approach, as I would like to focus on tokens, not on characters.\n\n* <b>chunk_size:</b> This parameter specifies the desired size of each chunk in terms of the number of tokens (words or subwords).\n* <b>chunk_overlap:</b> This parameter specifies the overlap between consecutive chunks. A value of 0 indicates no overlap, meaning each chunk will start exactly where the previous one ends.","metadata":{}},{"cell_type":"code","source":"'''splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,\n                                chunk_overlap = 20)\nsplitted_texts = splitter.split_documents(document)'''\n#Split text into smaller chunks\nsplitter = TokenTextSplitter(chunk_size=500, chunk_overlap=75)\nsplitted_texts = splitter.split_documents(document)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:45:39.441477Z","iopub.execute_input":"2024-04-09T13:45:39.442006Z","iopub.status.idle":"2024-04-09T13:45:40.619560Z","shell.execute_reply.started":"2024-04-09T13:45:39.441976Z","shell.execute_reply":"2024-04-09T13:45:40.618619Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Storing the data\n\nNow we need to put chunks into an index so that we are able to retrieve them easily when we want to find something in the document or answer questions. We use embedding model and vector database for this purpose.","metadata":{}},{"cell_type":"code","source":"#initialize an embedding model \nembedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n#create chroma database from provided documents\nchroma_database = Chroma.from_documents(splitted_texts,\n                                      embedding_model,\n                                      persist_directory = 'chroma_db')\n\n#convert chroma database into retriever\nretriever = chroma_database.as_retriever()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:45:40.620669Z","iopub.execute_input":"2024-04-09T13:45:40.621008Z","iopub.status.idle":"2024-04-09T13:45:52.058370Z","shell.execute_reply.started":"2024-04-09T13:45:40.620984Z","shell.execute_reply":"2024-04-09T13:45:52.057472Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"652df1774b544ceda37281e9e28f0be9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b99f9864613438996bfe65e1c224548"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42325f06b76f43b7a70a04249fe301fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9938acfab44423b9c00f72b016cc9be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06352b683a8e4e0182b689c576c3180b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f48a7e41aa543219e40cd79c05943dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f3167234a234c23b61a60bf8fea62b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d95c53babeef4190bb4bbe8539783f56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f4437050a1244dd842e07aaea68eab8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3e1d1bce09c4283a131cef53dd8b360"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"758e83526d0f4c0690cfdc7dfdff95d3"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Searching the articles\n\nI'll execute a similarity search using a query against a Chroma database to retrieve the most similar documents. I'll print information about the query and the retrieved documents, including their titles and snippets of text. Each search will give three highly related articles. The number of retrieved articles may be changed by changing 'k' parameter in similarity_search().","metadata":{}},{"cell_type":"code","source":"query = \"What is Word2Vec?\"\n\ndocs = chroma_database.similarity_search(query, k=3)\nprint(f'Query: {query}')\nprint(f'Retrieved documents: {len(docs)}')\nfor doc in docs:\n    details = doc.to_json()['kwargs']\n    print(\"\\nSource (Article Title):\", details['page_content'])\n    print(\"\\nText: \", details['metadata']['Text'][:350] + \". . .\")\n    print('\\n\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:45:52.059547Z","iopub.execute_input":"2024-04-09T13:45:52.060055Z","iopub.status.idle":"2024-04-09T13:45:52.106013Z","shell.execute_reply.started":"2024-04-09T13:45:52.060029Z","shell.execute_reply":"2024-04-09T13:45:52.105003Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Query: What is Word2Vec?\nRetrieved documents: 3\n\nSource (Article Title): A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model\n\nText:  1. Introduction of Word2vec\n\nWord2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to d. . .\n\n\n\n\n\nSource (Article Title): Analogies from Word Vectors?\n\nText:  Another common example of analogies is the derivation of capitals. Let’s see what are the candidates for the capital of Austria:\n\nCandidates for Austria-Germany+Berlin from German Wikipedia corpus\n\nThis time we have a hit, indeed Vienna is to Austria as Berlin is to Germany. The list of other candidates also makes sense, we find five capitals of Au. . .\n\n\n\n\n\nSource (Article Title): Word Clouds in Python: Comprehensive Example\n\nText:  Word Clouds in Python: Comprehensive Example\n\nImage from Wikipedia\n\nThis is a simple exercise visualizing my latest issue of The Economist in a word cloud. I wasn’t in the mood to actually read the issue, so I thought this would be a fun way to digest the information quickly.\n\nTable of Contents\n\nPDF to Text Conversion\n\nText Preprocessing\n\nWord Clou. . .\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"How to implement Gradient Descent and Backpropagation?\"\n\ndocs = chroma_database.similarity_search(query, k=3)\nprint(f'Query: {query}')\nprint(f'Retrieved documents: {len(docs)}')\nfor doc in docs:\n    details = doc.to_json()['kwargs']\n    print(\"\\nSource (Article Title):\", details['page_content'])\n    print(\"\\nText: \", details['metadata']['Text'][:350] + \". . .\")\n    print('\\n\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:45:52.109098Z","iopub.execute_input":"2024-04-09T13:45:52.109402Z","iopub.status.idle":"2024-04-09T13:45:52.184693Z","shell.execute_reply.started":"2024-04-09T13:45:52.109377Z","shell.execute_reply":"2024-04-09T13:45:52.183775Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Query: How to implement Gradient Descent and Backpropagation?\nRetrieved documents: 3\n\nSource (Article Title): A Step-by-Step Implementation of Gradient Descent and Backpropagation\n\nText:  A Step-by-Step Implementation of Gradient Descent and Backpropagation\n\nThe original intention behind this post was merely me brushing upon mathematics in neural network, as I like to be well versed in the inner workings of algorithms and get to the essence of things. I then think I might as well put together a story rather than just revisiting the . . .\n\n\n\n\n\nSource (Article Title): Understanding Backpropagation Algorithm\n\nText:  Backpropagation algorithm is probably the most fundamental building block in a neural network. It was first introduced in 1960s and almost 30 years later (1989) popularized by Rumelhart, Hinton and Williams in a paper called “Learning representations by back-propagating errors”.\n\nThe algorithm is used to effectively train a neural network through a. . .\n\n\n\n\n\nSource (Article Title): Understanding the Mathematics behind Gradient Descent.\n\nText:  “Premature optimization is the root of all evil.”\n\n― Donald Ervin Knuth\n\nAgile is a pretty well-known term in the software development process. The basic idea behind it is simple: build something quickly, ➡️ get it out there, ➡️ get some feedback ➡️ make changes depending upon the feedback ➡️ repeat the process. The goal is to get the product near . . .\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"Describe Logistic Regression\"\n\ndocs = chroma_database.similarity_search(query, k=3)\nprint(f'Query: {query}')\nprint(f'Retrieved documents: {len(docs)}')\nfor doc in docs:\n    details = doc.to_json()['kwargs']\n    print(\"\\nSource (Article Title):\", details['page_content'])\n    print(\"\\nText: \", details['metadata']['Text'][:350] + \". . .\")\n    print('\\n\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:45:52.186072Z","iopub.execute_input":"2024-04-09T13:45:52.186702Z","iopub.status.idle":"2024-04-09T13:45:52.208883Z","shell.execute_reply.started":"2024-04-09T13:45:52.186665Z","shell.execute_reply":"2024-04-09T13:45:52.208027Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Query: Describe Logistic Regression\nRetrieved documents: 3\n\nSource (Article Title): Logistic Regression\n\nText:  Logistic Regression\n\nContrary to its name logistic regression is a classification algorithm. Given an input example, a logistic regression model assigns the example to a relevant class.\n\nA note on the notation. x_{i} means x subscript i and x_{^th} means x superscript th.\n\nQuick Review of Linear Regression\n\nLinear Regression is used to predict a re. . .\n\n\n\n\n\nSource (Article Title): Logistic Regression in Machine Learning using Python\n\nText:  Logistic Regression in Machine Learning using Python\n\nLearn how logistic regression works and how you can easily implement it from scratch using python as well as using sklearn. Adarsh Menon · Follow Published in Towards Data Science · 7 min read · Dec 27, 2019 -- 2 Listen Share\n\nIn statistics logistic regression is used to model the probability of. . .\n\n\n\n\n\nSource (Article Title): Logistic Regression from Scratch in R\n\nText:  Introduction\n\nIn statistics and data science, logistic regression is used to predict the probability of a certain class or event. Usually, the model is binomial, but can also extend to multinomial. It probably is one of the simplest yet extremely useful models for a lot of applications, with its fast implementation and ease of interpretation.\n\nThis. . .\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"Why should I use Cython?\"\n\ndocs = chroma_database.similarity_search(query, k=3)\nprint(f'Query: {query}')\nprint(f'Retrieved documents: {len(docs)}')\nfor doc in docs:\n    details = doc.to_json()['kwargs']\n    print(\"\\nSource (Article Title):\", details['page_content'])\n    print(\"\\nText: \", details['metadata']['Text'][:350] + \". . .\")\n    print('\\n\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:45:52.209844Z","iopub.execute_input":"2024-04-09T13:45:52.210106Z","iopub.status.idle":"2024-04-09T13:45:52.232940Z","shell.execute_reply.started":"2024-04-09T13:45:52.210085Z","shell.execute_reply":"2024-04-09T13:45:52.231997Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Query: Why should I use Cython?\nRetrieved documents: 3\n\nSource (Article Title): Use Cython to get more than 30X speedup on your Python code\n\nText:  Cython will give your Python code super-car speed\n\nWant to be inspired? Come join my Super Quotes newsletter. 😎\n\nPython is a community favourite programming language! It’s by far one of the easiest to use as code is written in an intuitive, human-readable way.\n\nYet you’ll often hear the same complaint about Python over and over again, especially fr. . .\n\n\n\n\n\nSource (Article Title): PyTorch 1.3 — What’s new?. Support for Android and iOS, Named…\n\nText:  PyTorch 1.3 — What’s new?\n\nFacebook just released PyTorch v1.3 and it is packed with some of the most awaited features. The three most attractive ones are:\n\nNamed Tensor — Something that would make the life of machine learning practitioners much easier. Quantization — For performance critical systems like IoT devices and embedded systems. Mobile Su. . .\n\n\n\n\n\nSource (Article Title): Why is Python Programming a perfect fit for Big Data?\n\nText:  Why is Python Programming a perfect fit for Big Data?\n\nWe’ll discuss in the blog the major benefits of using Python for big data. Scarlett Rose · Follow 5 min read · Dec 27, 2019 -- Listen Share\n\nPython is everywhere!\n\nWith the widespread use of Python across major industry verticals, Python has become a hot topic of discussion in the town. Python . . .\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"Give me a list of examples where I can apply time series clustering\"\n\ndocs = chroma_database.similarity_search(query, k=3)\nprint(f'Query: {query}')\nprint(f'Retrieved documents: {len(docs)}')\nfor doc in docs:\n    details = doc.to_json()['kwargs']\n    print(\"\\nSource (Article Title):\", details['page_content'])\n    print(\"\\nText: \", details['metadata']['Text'][:350] + \". . .\")\n    print('\\n\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:45:52.234038Z","iopub.execute_input":"2024-04-09T13:45:52.234305Z","iopub.status.idle":"2024-04-09T13:45:52.293501Z","shell.execute_reply.started":"2024-04-09T13:45:52.234282Z","shell.execute_reply":"2024-04-09T13:45:52.292437Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Query: Give me a list of examples where I can apply time series clustering\nRetrieved documents: 3\n\nSource (Article Title): Time Series Clustering and Dimensionality Reduction\n\nText:  Time Series must be handled with care by data scientists. This kind of data contains intrinsic information about temporal dependency. it’s our work to extract these golden resources, where it is possible and useful, in order to help our model to perform the best.\n\nWith Time Series I see confusion when we face a problem of dimensionality reduction o. . .\n\n\n\n\n\nSource (Article Title): Working With Time Series Data\n\nText:  Working With Time Series Data\n\nNYC’s daily temperature chart (November 1, 2019 to December 11, 2019) produced with Matplotlib\n\nData scientists study time series data to determine if a time based trend exists. We can analyze hourly subway passengers, daily temperatures, monthly sales, and more to see if there are various types of trends. These trend. . .\n\n\n\n\n\nSource (Article Title): Cluster analysis: theory and implementation of unsupervised algorithms\n\nText:  In simple terms, clustering is nothing but separating observations based on certain properties. In a more technical term, clustering is an unsupervised machine learning algorithm, a process, by which observations (data) are grouped in a way that similar observations are put closer to each other. It is an “unsupervised” algorithm because unlike supe. . .\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Question-answering (QA) system\n\nMy QA system uses a prompt template for generating prompts, a language model chain for generating answers, and a retrieval-augmented generation chain to combine retrieval and generation approaches in the QA process.\n\n* <b>llm_chain:</b> sets up a generation-based question-answering chain using the LLMChain class. It uses the language model (llm) and the same prompt template. It doesn't use retriever.\n* <b>rag_chain:</b> sets up a retrieval-augmented generation (RAG) chain. It combines retrieval (using the retriever) with generation (using the language model).\n\nAs we can see on the example regarding the logistic regression, the answers with RAG may be more precise.","metadata":{}},{"cell_type":"code","source":"#define a configuration for quantization \nquantization_config = BitsAndBytesConfig(load_in_4bit = True,\n                                        bnb_4bit_quant_type = 'nf4',\n                                        bnb_4bit_use_double_quant = True,\n                                        bnb_4bit_compute_dtype = bfloat16)\n\n#create a text generation pipeline using Mistral7b\ntext_generation_pipeline = transformers.pipeline(\n    model=model,\n    tokenizer=tokenizer,\n    task=\"text-generation\",\n    temperature=0.2,\n    repetition_penalty=1.1,\n    return_full_text=True,\n    max_new_tokens=300,\n)\n\nllm = HuggingFacePipeline(pipeline=text_generation_pipeline)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:45:52.294674Z","iopub.execute_input":"2024-04-09T13:45:52.295004Z","iopub.status.idle":"2024-04-09T13:46:07.574730Z","shell.execute_reply.started":"2024-04-09T13:45:52.294978Z","shell.execute_reply":"2024-04-09T13:46:07.573894Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"2024-04-09 13:45:55.761398: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-09 13:45:55.761540: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-09 13:45:56.018657: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# build prompt\nprompt_template = \"\"\"\n### [INST] \nInstruction: Answer the question based on your data science knowledge. Here is context to help:\n\n{context}\n\n### QUESTION:\n{question} \n\n[/INST]\"\"\"\n\n# create prompt from prompt template \nprompt = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=prompt_template,\n)\n\n# create llm chain \nllm_chain = LLMChain(llm=llm, prompt=prompt)\n\n# create rag chain \nrag_chain = ( \n {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | llm_chain\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:46:07.575885Z","iopub.execute_input":"2024-04-09T13:46:07.576153Z","iopub.status.idle":"2024-04-09T13:46:07.583340Z","shell.execute_reply.started":"2024-04-09T13:46:07.576130Z","shell.execute_reply":"2024-04-09T13:46:07.582292Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"query = \"What is Word2Vec?\"\nprint('Query: '+query)\nprint('\\n')\nprint('Answer: '+llm_chain.invoke({\"context\":\"\", \n                  \"question\": query})['text'].split('[/INST]')[1].strip())\nprint('\\n\\n')\nprint('Answer with RAG: '+rag_chain.invoke(query)['text'].split('[/INST]')[1].strip())","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:46:07.584790Z","iopub.execute_input":"2024-04-09T13:46:07.585178Z","iopub.status.idle":"2024-04-09T13:47:11.055604Z","shell.execute_reply.started":"2024-04-09T13:46:07.585144Z","shell.execute_reply":"2024-04-09T13:47:11.054644Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Query: What is Word2Vec?\n\n\nAnswer: Word2Vec is a natural language processing technique used for representing words as vectors in a high-dimensional space. It is a type of neural network that learns word embeddings, which are dense, low-dimensional representations of words that capture semantic relationships between them. The idea behind Word2Vec is to map words to continuous vectors such that the similarity between two words can be measured by the distance between their corresponding vectors in the vector space. This allows for more efficient and accurate text classification, sentiment analysis, machine translation, and other NLP tasks.\n\n\n\nAnswer with RAG: Word2Vec is a neural network architecture that is commonly used for learning word embeddings, which are dense vector representations of words that capture semantic relationships between them. Word2Vec uses a two-layer neural network to map words to vectors in a high-dimensional space. The input to the network is a corpus of text, and the output is a set of word vectors. There are two main training algorithms for Word2Vec: CBOW and Skip-gram. CBOW trains the network to predict a target word given its context words, while Skip-gram trains the network to predict a target context given a target word. Word2Vec has been widely used in various NLP tasks, including sentiment analysis, named entity recognition, and machine translation.\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"How to implement Gradient Descent and Backpropagation?\"\nprint('Query: '+query)\nprint('\\n')\nprint('Answer: '+llm_chain.invoke({\"context\":\"\", \n                  \"question\": query})['text'].split('[/INST]')[1].strip())\nprint('\\n\\n')\nprint('Answer with RAG: '+rag_chain.invoke(query)['text'].split('[/INST]')[1].strip())","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:47:11.056898Z","iopub.execute_input":"2024-04-09T13:47:11.057210Z","iopub.status.idle":"2024-04-09T13:48:28.301808Z","shell.execute_reply.started":"2024-04-09T13:47:11.057168Z","shell.execute_reply":"2024-04-09T13:48:28.300790Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Query: How to implement Gradient Descent and Backpropagation?\n\n\nAnswer: Gradient descent and backpropagation are two fundamental techniques used in machine learning for optimizing models. Here's a step-by-step guide on how to implement them:\n\n1. Define the objective function: The first step is to define the objective function that you want to minimize or maximize. This could be the cost function of a linear regression model, the cross-entropy loss of a neural network, or any other function that measures the performance of your model.\n2. Initialize the weights: Next, initialize the weights of your model randomly or using some pre-trained values. The weights represent the parameters of your model and will be updated during training.\n3. Choose the learning rate: The learning rate is a hyperparameter that determines the step size at each iteration of gradient descent. A small learning rate may result in slow convergence, while a large learning rate may cause the model to diverge. You can experiment with different learning rates to find the optimal value.\n4. Iterate over the training data: For each batch of training data, compute the forward pass of your model to obtain the predicted outputs. Then, compute the error between the predicted outputs and the true labels.\n5. Compute the gradients: Use the chain rule of calculus to compute the gradients of the objective function with respect to the weights. These gradients represent the direction of steepest descent of the objective function and will be used to update the weights.\n\n\n\nAnswer with RAG: Gradient descent and backpropagation are two important techniques used in machine learning, particularly in neural networks. Gradient descent is an optimization algorithm that minimizes the difference between the predicted output of a model and the true output. Backpropagation is a technique used to train neural networks by adjusting the weights of the connections between neurons based on the gradient of the loss function with respect to the weights.\n\nHere is a general outline of how to implement gradient descent and backpropagation:\n\n1. Initialize the weights of the neural network randomly.\n2. Define the loss function, which measures the difference between the predicted output and the true output.\n3. Choose an initial learning rate, which determines the step size taken in each iteration of gradient descent.\n4. Iteratively update the weights of the neural network using gradient descent until convergence. At each iteration, compute the gradient of the loss function with respect to the weights, and update the weights by taking a step in the direction of the negative gradient.\n5. Use backpropagation to adjust the weights of the neural network based on the gradient of the loss function with respect to the weights. At each iteration, compute the gradient of the loss function with respect to the weights, and update the weights by taking a step in the direction of the negative gradient.\n\nIt's worth noting that there are many variations and extensions of gradient descent and backpropagation, such as stochastic gradient descent, mini-\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"Describe Logistic Regression\"\nprint('Query: '+query)\nprint('\\n')\nprint('Answer: '+llm_chain.invoke({\"context\":\"\", \n                  \"question\": query})['text'].split('[/INST]')[1].strip())\nprint('\\n\\n')\nprint('Answer with RAG: '+rag_chain.invoke(query)['text'].split('[/INST]')[1].strip())","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:48:28.302984Z","iopub.execute_input":"2024-04-09T13:48:28.303291Z","iopub.status.idle":"2024-04-09T13:50:09.169274Z","shell.execute_reply.started":"2024-04-09T13:48:28.303266Z","shell.execute_reply":"2024-04-09T13:50:09.168296Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Query: Describe Logistic Regression\n\n\nAnswer: Logistic regression is a type of supervised learning algorithm used for classification problems, where the goal is to predict the probability of an event occurring based on a set of input features. It is a statistical method that uses a logistic function to model the relationship between the input variables and the output variable. The logistic function maps any input value to a value between 0 and 1, which can be interpreted as the probability of the event occurring.\n\nIn logistic regression, the model is trained by minimizing the cross-entropy loss function, which measures the difference between the predicted probabilities and the true labels. The coefficients of the logistic function are learned during training, and they represent the impact of each input feature on the probability of the event occurring. Once the model is trained, it can be used to make predictions on new data by applying the logistic function to the input values and interpreting the resulting probabilities.\n\nLogistic regression is a widely used and effective algorithm for classification problems, particularly when the relationships between the input variables and the output variable are linear or nearly linear. However, it may not perform well when the relationships are more complex or nonlinear, in which case other algorithms such as decision trees, random forests, or neural networks may be more appropriate.\n\n\n\nAnswer with RAG: Logistic Regression is a type of supervised learning algorithm used for classification problems. It is a statistical method that uses a logistic function to model the probability of a binary response variable. The logistic function maps any real-valued input to a value between 0 and 1, which can be interpreted as the probability of the response variable taking on a particular value.\n\nIn logistic regression, the logistic function is used to model the probability of a binary response variable. The logistic function takes a linear combination of input features and produces a probability value between 0 and 1. The logistic function is defined as follows:\n\nP(y=1) = 1 / (1 + e^-z)\n\nWhere P(y=1) is the probability of the response variable taking on the value 1, y is the response variable, and z is the linear combination of input features.\n\nThe logistic regression model is trained by minimizing the log-loss function, which measures the difference between the predicted probabilities and the true class labels. The log-loss function is defined as follows:\n\nL(w) = -1/N \\* sum(y\\_i \\* log(P(y\\_i=1)) + (1-y\\_i) \\* log(1-P(y\\_i=1)))\n\nWhere N is the number of samples, y\\_i is\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"Why should I use Cython?\"\nprint('Query: '+query)\nprint('\\n')\nprint('Answer: '+llm_chain.invoke({\"context\":\"\", \n                  \"question\": query})['text'].split('[/INST]')[1].strip())\nprint('\\n\\n')\nprint('Answer with RAG: '+rag_chain.invoke(query)['text'].split('[/INST]')[1].strip())","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:50:09.170614Z","iopub.execute_input":"2024-04-09T13:50:09.171019Z","iopub.status.idle":"2024-04-09T13:51:12.404641Z","shell.execute_reply.started":"2024-04-09T13:50:09.170982Z","shell.execute_reply":"2024-04-09T13:51:12.403765Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Query: Why should I use Cython?\n\n\nAnswer: Cython is a language that allows you to write Python code that can be compiled into machine code, which can run much faster than pure Python code. This can be useful for performance-critical applications where speed is important, such as scientific computing or data analysis.\n\nOne of the main advantages of using Cython is that it allows you to take advantage of the performance benefits of C and C++ without having to learn those languages. Cython is similar to Python in many ways, but it also has some features that are specific to C and C++, such as support for pointers and low-level memory management.\n\nAnother advantage of using Cython is that it can be used with popular Python libraries such as NumPy and Pandas, which can significantly improve the performance of these libraries when working with large datasets.\n\nOverall, Cython can be a powerful tool for improving the performance of Python code, especially for applications that require high levels of speed and efficiency.\n\n\n\nAnswer with RAG: Cython is a programming language that allows you to write Python code with some minor modifications, which is then translated directly into C code. This allows you to speed up your raw Python code by taking advantage of the performance characteristics of C. Cython is particularly useful for situations where you have a pure Python loop that you want to speed up, as it can provide a significant performance boost. Cython is also useful for situations where you need to use external libraries that are not available in Python, as you can use Cython to call those libraries directly from Python.\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"Give me a list of examples where I can apply time series clustering\"\nprint('Query: '+query)\nprint('\\n')\nprint('Answer: '+llm_chain.invoke({\"context\":\"\", \n                  \"question\": query})['text'].split('[/INST]')[1].strip())\nprint('\\n\\n')\nprint('Answer with RAG: '+rag_chain.invoke(query)['text'].split('[/INST]')[1].strip())","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:51:12.405745Z","iopub.execute_input":"2024-04-09T13:51:12.406015Z","iopub.status.idle":"2024-04-09T13:52:31.621508Z","shell.execute_reply.started":"2024-04-09T13:51:12.405993Z","shell.execute_reply":"2024-04-09T13:52:31.620526Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Query: Give me a list of examples where I can apply time series clustering\n\n\nAnswer: Time series clustering is a technique used in data science to group similar time series data together based on their patterns and trends over time. Here are some examples of where you can apply time series clustering:\n\n1. Stock market analysis: You can use time series clustering to group stocks with similar price movements together, allowing for better investment strategies.\n2. Customer behavior analysis: By analyzing customer purchase patterns over time, you can cluster customers into different segments based on their buying habits, which can be useful for marketing and sales purposes.\n3. Weather forecasting: Time series clustering can be used to group weather data from different locations together based on their similarities, which can help improve weather forecasting accuracy.\n4. Healthcare monitoring: By analyzing patient health data over time, you can cluster patients with similar health conditions together, which can help healthcare providers develop more effective treatment plans.\n5. Energy consumption analysis: Time series clustering can be used to group energy consumption data from different buildings or industries together based on their similarities, which can help identify areas for energy efficiency improvements.\n\n\n\nAnswer with RAG: Time series clustering can be applied in various fields and industries, including:\n\n1. Finance: Clustering financial time series data can help identify groups of stocks with similar behavior, which can aid in portfolio management and risk assessment.\n2. Healthcare: Time series clustering can be used to group patients based on their health status, medical history, and treatment outcomes, helping healthcare professionals develop personalized care plans.\n3. Marketing: By analyzing customer behavior and purchase patterns, time series clustering can help businesses segment their customers into different groups for targeted marketing campaigns.\n4. Environmental sciences: Time series clustering can be used to group weather patterns, air quality data, and other environmental factors to better understand and predict climate change and its impacts.\n5. Energy production: Clustering energy consumption data can help identify patterns and optimize energy usage in buildings and industrial facilities.\n6. Manufacturing: Time series clustering can be applied to production data to identify trends, improve supply chain management, and optimize manufacturing processes.\n7. Sports analytics: Analyzing time series data from athletic performances can help coaches and athletes identify areas for improvement and develop training programs tailored to individual needs.\n","output_type":"stream"}]}]}